{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "path = \"alt.atheism/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2\n",
    "\n",
    "Download and pre-process a large, publicly available text corpus. The preprocessing includes tokenization, normalization (e.g. convert to lowercase, remove non-alphanumeric chars, numbers, etc.). Output the preprocessing results as a text file, each line containing a single document (e.g. news article, sentence, yelp review, etc), with normalized tokens separated by a single white space. For example, the output file could look something like this:\n",
    "\n",
    "_this is my first normalized sentence or document_\n",
    "\n",
    "_this is my second sentence note that there are no numbers or punctuation_\n",
    "\n",
    "_this is my third example note that all tokens are separated by a single white space_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for file in os.listdir(path):\n",
    "    f = open(path + file, 'r', encoding = 'utf8', errors='ignore')\n",
    "    text.append(f.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for t in text:\n",
    "    tokenized.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "for tokenized_text in tokenized:\n",
    "    alphanum = [i.lower() for i in tokenized_text if i.isalpha()]\n",
    "    normalized.append(alphanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    for doc in normalized:\n",
    "        for token in doc:\n",
    "            f.write(token+' ')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word2vec embeddings of your corpus using the original word2vec C implementation (https://code.google.com/archive/p/word2vec/ (Links to an external site.), https://github.com/tmikolov/word2vec (Links to an external site.)) or the python gensim implementation of the same algorithm (https://radimrehurek.com/gensim/models/word2vec.html (Links to an external site.)). Note that the gensim library can load/use word2vec embeddings generated with the original word2vec C implementation, which appears to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_win5_emb100 = Word2Vec(normalized, size=100, window=5, min_count=3, sg=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.50116229e-01, -5.33902384e-02, -3.32647711e-01, -7.09465444e-01,\n",
       "        3.30302417e-02,  9.18757319e-02, -1.21799827e-01, -2.23813251e-01,\n",
       "       -1.32582396e-01,  8.81363273e-01,  3.31485689e-01,  1.53055191e-01,\n",
       "       -5.61688364e-01, -4.53914642e-01,  2.02939585e-01, -1.16395831e+00,\n",
       "        7.08374977e-01, -3.86702061e-01, -4.22304690e-01,  1.31783515e-01,\n",
       "       -2.98752993e-01, -5.07760704e-01, -3.16906045e-03,  1.25914633e+00,\n",
       "        6.23812806e-03,  2.30205521e-01,  1.80585086e-01,  4.97592449e-01,\n",
       "       -2.18527988e-01, -4.60152090e-01,  8.76268864e-01,  4.30669822e-02,\n",
       "        3.19028169e-01,  5.91184676e-01,  4.22868103e-01, -5.17717786e-02,\n",
       "        5.96705496e-01, -5.77265799e-01,  1.35623083e-01,  5.53592503e-01,\n",
       "        2.40148365e-01,  6.57165766e-01, -6.00041091e-01,  6.86857700e-02,\n",
       "       -8.86012614e-01, -3.79629105e-01, -3.27166051e-01,  1.09146774e+00,\n",
       "        9.13431227e-01, -1.16205618e-01,  3.45087983e-02,  2.12658361e-01,\n",
       "        9.76695865e-02, -4.58143562e-01, -1.02109320e-01, -7.36812472e-01,\n",
       "        2.28710249e-01, -1.68215144e+00,  2.43755952e-01,  2.85682857e-01,\n",
       "        1.81526735e-01, -6.17605507e-01, -1.34589002e-01, -8.75754356e-01,\n",
       "        7.83334315e-01, -5.16327918e-01,  9.95173119e-04,  3.95884603e-01,\n",
       "        7.40354776e-01,  8.50198716e-02,  6.49927139e-01, -2.78697759e-01,\n",
       "        1.56034589e-01,  9.18000519e-01,  4.17130813e-02,  1.35118470e-01,\n",
       "       -5.87477803e-01,  2.00329006e-01,  6.18624449e-01,  4.22207713e-01,\n",
       "       -5.26947558e-01, -9.42941308e-01,  8.06850731e-01, -1.62079707e-01,\n",
       "        5.17942429e-01, -1.59143843e-02,  2.03752592e-01, -7.39929616e-01,\n",
       "        1.06394365e-01, -5.77577502e-02, -9.54269171e-01,  1.45601938e-02,\n",
       "       -5.48292734e-02,  5.50734878e-01, -6.24668784e-02, -2.42474049e-01,\n",
       "        1.01223096e-01, -1.63481727e-01, -5.31364024e-01, -4.00470831e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = cbow_win5_emb100.wv['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "After completing the lab assignment, experiment with 2 or more sets of word2vec model parameters, for example, different embedding sizes, CBOW vs. skip-gram models, etc. Provide a paragraph or two of qualitative evaluation of your embeddings (no need to provide a quantitative evaluation). For example, you can evaluate the embeddings by hand-picking ~10 words and manually reviewing/evaluating their closest neighbors in terms of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec using CBOW, window size 5, 100 embedding size takes 1.9530611038208008\n",
      "word2vec using CBOW, window size 10, 150 embedding size takes 1.988832950592041\n",
      "word2vec using skip-gram, window size 5, 100 embedding size takes 2.966956853866577\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cbow_win5_emb100 = Word2Vec(normalized, size=100, window=5, min_count=3, sg=0, workers=4)\n",
    "print(\"word2vec using CBOW, window size 5, 100 embedding size takes %s\"%(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "cbow_win10_emb150 = Word2Vec(normalized, size=150, window=10, min_count=3, sg=0, workers=4)\n",
    "print(\"word2vec using CBOW, window size 10, 150 embedding size takes %s\"%(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "skip_win5_emb100 = Word2Vec(normalized, size=100, window=5, min_count=3, sg=1, workers=4)\n",
    "print(\"word2vec using skip-gram, window size 5, 100 embedding size takes %s\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_most_words(model, target_words, num_of_words=5):\n",
    "    res = {}\n",
    "    for i in target_words:\n",
    "        res[i] = model.wv.most_similar(i)[:num_of_words]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = ['god', 'human', 'science', 'data', 'earth', 'play', 'physics', 'computer', 'study', 'myth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>god</th>\n",
       "      <th>human</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>earth</th>\n",
       "      <th>play</th>\n",
       "      <th>physics</th>\n",
       "      <th>computer</th>\n",
       "      <th>study</th>\n",
       "      <th>myth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(fact, 0.9652177095413208)</td>\n",
       "      <td>(statements, 0.9898587465286255)</td>\n",
       "      <td>(basis, 0.9508533477783203)</td>\n",
       "      <td>(traditional, 0.9968911409378052)</td>\n",
       "      <td>(age, 0.9851472973823547)</td>\n",
       "      <td>(directly, 0.9976117610931396)</td>\n",
       "      <td>(supporting, 0.9892902374267578)</td>\n",
       "      <td>(dept, 0.9969635605812073)</td>\n",
       "      <td>(together, 0.9978960156440735)</td>\n",
       "      <td>(scale, 0.9945277571678162)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(one, 0.9467852115631104)</td>\n",
       "      <td>(unreasonable, 0.9890013337135315)</td>\n",
       "      <td>(steps, 0.9433700442314148)</td>\n",
       "      <td>(party, 0.9964227080345154)</td>\n",
       "      <td>(states, 0.9837836623191833)</td>\n",
       "      <td>(regarding, 0.9973920583724976)</td>\n",
       "      <td>(rule, 0.9891491532325745)</td>\n",
       "      <td>(wisconsin, 0.9935089945793152)</td>\n",
       "      <td>(stand, 0.9971149563789368)</td>\n",
       "      <td>(seven, 0.9939987659454346)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(question, 0.9437123537063599)</td>\n",
       "      <td>(without, 0.9864014387130737)</td>\n",
       "      <td>(alterius, 0.9377532005310059)</td>\n",
       "      <td>(per, 0.9959977865219116)</td>\n",
       "      <td>(planets, 0.9826770424842834)</td>\n",
       "      <td>(full, 0.9971146583557129)</td>\n",
       "      <td>(imho, 0.9882904291152954)</td>\n",
       "      <td>(austin, 0.9915690422058105)</td>\n",
       "      <td>(unlike, 0.9964526295661926)</td>\n",
       "      <td>(reviewette, 0.993532657623291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(gods, 0.9388728141784668)</td>\n",
       "      <td>(life, 0.9859429001808167)</td>\n",
       "      <td>(realm, 0.9364252090454102)</td>\n",
       "      <td>(invalidates, 0.9957599639892578)</td>\n",
       "      <td>(condemned, 0.9823474287986755)</td>\n",
       "      <td>(sp, 0.9970144629478455)</td>\n",
       "      <td>(length, 0.9878376722335815)</td>\n",
       "      <td>(cs, 0.9915235638618469)</td>\n",
       "      <td>(short, 0.9964014887809753)</td>\n",
       "      <td>(dark, 0.9933671355247498)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(statement, 0.9352432489395142)</td>\n",
       "      <td>(contents, 0.983342170715332)</td>\n",
       "      <td>(atheism, 0.9351377487182617)</td>\n",
       "      <td>(goodness, 0.9956218600273132)</td>\n",
       "      <td>(disciples, 0.9811902046203613)</td>\n",
       "      <td>(unlike, 0.9966237545013428)</td>\n",
       "      <td>(bug, 0.9868855476379395)</td>\n",
       "      <td>(technology, 0.9909390807151794)</td>\n",
       "      <td>(instead, 0.9963665008544922)</td>\n",
       "      <td>(couple, 0.9933009147644043)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               god                               human  \\\n",
       "0       (fact, 0.9652177095413208)    (statements, 0.9898587465286255)   \n",
       "1        (one, 0.9467852115631104)  (unreasonable, 0.9890013337135315)   \n",
       "2   (question, 0.9437123537063599)       (without, 0.9864014387130737)   \n",
       "3       (gods, 0.9388728141784668)          (life, 0.9859429001808167)   \n",
       "4  (statement, 0.9352432489395142)       (contents, 0.983342170715332)   \n",
       "\n",
       "                          science                               data  \\\n",
       "0     (basis, 0.9508533477783203)  (traditional, 0.9968911409378052)   \n",
       "1     (steps, 0.9433700442314148)        (party, 0.9964227080345154)   \n",
       "2  (alterius, 0.9377532005310059)          (per, 0.9959977865219116)   \n",
       "3     (realm, 0.9364252090454102)  (invalidates, 0.9957599639892578)   \n",
       "4   (atheism, 0.9351377487182617)     (goodness, 0.9956218600273132)   \n",
       "\n",
       "                             earth                             play  \\\n",
       "0        (age, 0.9851472973823547)   (directly, 0.9976117610931396)   \n",
       "1     (states, 0.9837836623191833)  (regarding, 0.9973920583724976)   \n",
       "2    (planets, 0.9826770424842834)       (full, 0.9971146583557129)   \n",
       "3  (condemned, 0.9823474287986755)         (sp, 0.9970144629478455)   \n",
       "4  (disciples, 0.9811902046203613)     (unlike, 0.9966237545013428)   \n",
       "\n",
       "                            physics                          computer  \\\n",
       "0  (supporting, 0.9892902374267578)        (dept, 0.9969635605812073)   \n",
       "1        (rule, 0.9891491532325745)   (wisconsin, 0.9935089945793152)   \n",
       "2        (imho, 0.9882904291152954)      (austin, 0.9915690422058105)   \n",
       "3      (length, 0.9878376722335815)          (cs, 0.9915235638618469)   \n",
       "4         (bug, 0.9868855476379395)  (technology, 0.9909390807151794)   \n",
       "\n",
       "                            study                             myth  \n",
       "0  (together, 0.9978960156440735)      (scale, 0.9945277571678162)  \n",
       "1     (stand, 0.9971149563789368)      (seven, 0.9939987659454346)  \n",
       "2    (unlike, 0.9964526295661926)  (reviewette, 0.993532657623291)  \n",
       "3     (short, 0.9964014887809753)       (dark, 0.9933671355247498)  \n",
       "4   (instead, 0.9963665008544922)     (couple, 0.9933009147644043)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(get_similar_most_words(cbow_win5_emb100, target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>god</th>\n",
       "      <th>human</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>earth</th>\n",
       "      <th>play</th>\n",
       "      <th>physics</th>\n",
       "      <th>computer</th>\n",
       "      <th>study</th>\n",
       "      <th>myth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(existence, 0.9629144072532654)</td>\n",
       "      <td>(being, 0.9938219785690308)</td>\n",
       "      <td>(origins, 0.9624210000038147)</td>\n",
       "      <td>(violation, 0.9971508979797363)</td>\n",
       "      <td>(levelled, 0.9902923107147217)</td>\n",
       "      <td>(supports, 0.9984139800071716)</td>\n",
       "      <td>(fiction, 0.9850502014160156)</td>\n",
       "      <td>(carnegie, 0.9936286211013794)</td>\n",
       "      <td>(chip, 0.9975271224975586)</td>\n",
       "      <td>(eltnc, 0.9979461431503296)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(fact, 0.9466567039489746)</td>\n",
       "      <td>(very, 0.9900756478309631)</td>\n",
       "      <td>(basis, 0.9295337200164795)</td>\n",
       "      <td>(miracles, 0.9965916872024536)</td>\n",
       "      <td>(burden, 0.9890457391738892)</td>\n",
       "      <td>(translation, 0.9970253705978394)</td>\n",
       "      <td>(jkj, 0.9847123622894287)</td>\n",
       "      <td>(university, 0.9920341372489929)</td>\n",
       "      <td>(recognize, 0.9971242547035217)</td>\n",
       "      <td>(mcvey, 0.997639000415802)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(belief, 0.9369016885757446)</td>\n",
       "      <td>(sides, 0.9892735481262207)</td>\n",
       "      <td>(hqd, 0.9100735187530518)</td>\n",
       "      <td>(nec, 0.9964849948883057)</td>\n",
       "      <td>(states, 0.9890228509902954)</td>\n",
       "      <td>(assertions, 0.9968380928039551)</td>\n",
       "      <td>(k, 0.9827766418457031)</td>\n",
       "      <td>(mst, 0.9888726472854614)</td>\n",
       "      <td>(myths, 0.9961103200912476)</td>\n",
       "      <td>(eng, 0.9974782466888428)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(evidence, 0.9254992008209229)</td>\n",
       "      <td>(simply, 0.9888495802879333)</td>\n",
       "      <td>(waterloo, 0.9080314636230469)</td>\n",
       "      <td>(ban, 0.9957883358001709)</td>\n",
       "      <td>(particle, 0.9885709285736084)</td>\n",
       "      <td>(theology, 0.9964954257011414)</td>\n",
       "      <td>(length, 0.9818565845489502)</td>\n",
       "      <td>(keywords, 0.9870120882987976)</td>\n",
       "      <td>(pragmatic, 0.9953621625900269)</td>\n",
       "      <td>(leibowitz, 0.9972885847091675)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(manifestation, 0.9245367050170898)</td>\n",
       "      <td>(work, 0.9876078367233276)</td>\n",
       "      <td>(irving, 0.9015686511993408)</td>\n",
       "      <td>(incompleteness, 0.9957283735275269)</td>\n",
       "      <td>(esp, 0.9877588152885437)</td>\n",
       "      <td>(significant, 0.996310293674469)</td>\n",
       "      <td>(separates, 0.9814780950546265)</td>\n",
       "      <td>(computing, 0.9862393736839294)</td>\n",
       "      <td>(stop, 0.9951556921005249)</td>\n",
       "      <td>(whyalla, 0.9972806572914124)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   god                         human  \\\n",
       "0      (existence, 0.9629144072532654)   (being, 0.9938219785690308)   \n",
       "1           (fact, 0.9466567039489746)    (very, 0.9900756478309631)   \n",
       "2         (belief, 0.9369016885757446)   (sides, 0.9892735481262207)   \n",
       "3       (evidence, 0.9254992008209229)  (simply, 0.9888495802879333)   \n",
       "4  (manifestation, 0.9245367050170898)    (work, 0.9876078367233276)   \n",
       "\n",
       "                          science                                  data  \\\n",
       "0   (origins, 0.9624210000038147)       (violation, 0.9971508979797363)   \n",
       "1     (basis, 0.9295337200164795)        (miracles, 0.9965916872024536)   \n",
       "2       (hqd, 0.9100735187530518)             (nec, 0.9964849948883057)   \n",
       "3  (waterloo, 0.9080314636230469)             (ban, 0.9957883358001709)   \n",
       "4    (irving, 0.9015686511993408)  (incompleteness, 0.9957283735275269)   \n",
       "\n",
       "                            earth                               play  \\\n",
       "0  (levelled, 0.9902923107147217)     (supports, 0.9984139800071716)   \n",
       "1    (burden, 0.9890457391738892)  (translation, 0.9970253705978394)   \n",
       "2    (states, 0.9890228509902954)   (assertions, 0.9968380928039551)   \n",
       "3  (particle, 0.9885709285736084)     (theology, 0.9964954257011414)   \n",
       "4       (esp, 0.9877588152885437)   (significant, 0.996310293674469)   \n",
       "\n",
       "                           physics                          computer  \\\n",
       "0    (fiction, 0.9850502014160156)    (carnegie, 0.9936286211013794)   \n",
       "1        (jkj, 0.9847123622894287)  (university, 0.9920341372489929)   \n",
       "2          (k, 0.9827766418457031)         (mst, 0.9888726472854614)   \n",
       "3     (length, 0.9818565845489502)    (keywords, 0.9870120882987976)   \n",
       "4  (separates, 0.9814780950546265)   (computing, 0.9862393736839294)   \n",
       "\n",
       "                             study                             myth  \n",
       "0       (chip, 0.9975271224975586)      (eltnc, 0.9979461431503296)  \n",
       "1  (recognize, 0.9971242547035217)       (mcvey, 0.997639000415802)  \n",
       "2      (myths, 0.9961103200912476)        (eng, 0.9974782466888428)  \n",
       "3  (pragmatic, 0.9953621625900269)  (leibowitz, 0.9972885847091675)  \n",
       "4       (stop, 0.9951556921005249)    (whyalla, 0.9972806572914124)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(get_similar_most_words(cbow_win10_emb150, target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>god</th>\n",
       "      <th>human</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>earth</th>\n",
       "      <th>play</th>\n",
       "      <th>physics</th>\n",
       "      <th>computer</th>\n",
       "      <th>study</th>\n",
       "      <th>myth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(satan, 0.7217923998832703)</td>\n",
       "      <td>(intelligence, 0.9144315719604492)</td>\n",
       "      <td>(percent, 0.6613577008247375)</td>\n",
       "      <td>(board, 0.948987603187561)</td>\n",
       "      <td>(orbit, 0.8593956232070923)</td>\n",
       "      <td>(pick, 0.9560073614120483)</td>\n",
       "      <td>(pihatie, 0.7993094325065613)</td>\n",
       "      <td>(carnegie, 0.9562980532646179)</td>\n",
       "      <td>(differences, 0.9717433452606201)</td>\n",
       "      <td>(illiad, 0.9435649514198303)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(exist, 0.6900628805160522)</td>\n",
       "      <td>(processes, 0.9011027812957764)</td>\n",
       "      <td>(guaranteed, 0.6345124244689941)</td>\n",
       "      <td>(medical, 0.9306854605674744)</td>\n",
       "      <td>(starts, 0.8581326007843018)</td>\n",
       "      <td>(refute, 0.9553372859954834)</td>\n",
       "      <td>(rule, 0.7873783707618713)</td>\n",
       "      <td>(dept, 0.9539111256599426)</td>\n",
       "      <td>(draw, 0.9691357016563416)</td>\n",
       "      <td>(relevance, 0.9393407106399536)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(believing, 0.6897610425949097)</td>\n",
       "      <td>(individuals, 0.8980531096458435)</td>\n",
       "      <td>(value, 0.6306748390197754)</td>\n",
       "      <td>(experimental, 0.9188069105148315)</td>\n",
       "      <td>(heavens, 0.8402745127677917)</td>\n",
       "      <td>(escapes, 0.9544968605041504)</td>\n",
       "      <td>(boston, 0.7608155608177185)</td>\n",
       "      <td>(tech, 0.9470431208610535)</td>\n",
       "      <td>(changing, 0.9681685566902161)</td>\n",
       "      <td>(jerry, 0.9353609085083008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(existence, 0.6873213052749634)</td>\n",
       "      <td>(result, 0.897000253200531)</td>\n",
       "      <td>(computer, 0.6268389225006104)</td>\n",
       "      <td>(preservation, 0.9112571477890015)</td>\n",
       "      <td>(night, 0.7679082751274109)</td>\n",
       "      <td>(literally, 0.9532182216644287)</td>\n",
       "      <td>(game, 0.7527147531509399)</td>\n",
       "      <td>(mellon, 0.9383187890052795)</td>\n",
       "      <td>(happiness, 0.9679674506187439)</td>\n",
       "      <td>(undisputed, 0.9350335001945496)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(exists, 0.6844085454940796)</td>\n",
       "      <td>(relative, 0.8929824233055115)</td>\n",
       "      <td>(mixture, 0.6199126839637756)</td>\n",
       "      <td>(northern, 0.9107797145843506)</td>\n",
       "      <td>(luke, 0.7593796253204346)</td>\n",
       "      <td>(trap, 0.9523584842681885)</td>\n",
       "      <td>(ditto, 0.7299305200576782)</td>\n",
       "      <td>(virginia, 0.932227611541748)</td>\n",
       "      <td>(reflect, 0.9678720235824585)</td>\n",
       "      <td>(glory, 0.9347180128097534)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               god                               human  \\\n",
       "0      (satan, 0.7217923998832703)  (intelligence, 0.9144315719604492)   \n",
       "1      (exist, 0.6900628805160522)     (processes, 0.9011027812957764)   \n",
       "2  (believing, 0.6897610425949097)   (individuals, 0.8980531096458435)   \n",
       "3  (existence, 0.6873213052749634)         (result, 0.897000253200531)   \n",
       "4     (exists, 0.6844085454940796)      (relative, 0.8929824233055115)   \n",
       "\n",
       "                            science                                data  \\\n",
       "0     (percent, 0.6613577008247375)          (board, 0.948987603187561)   \n",
       "1  (guaranteed, 0.6345124244689941)       (medical, 0.9306854605674744)   \n",
       "2       (value, 0.6306748390197754)  (experimental, 0.9188069105148315)   \n",
       "3    (computer, 0.6268389225006104)  (preservation, 0.9112571477890015)   \n",
       "4     (mixture, 0.6199126839637756)      (northern, 0.9107797145843506)   \n",
       "\n",
       "                           earth                             play  \\\n",
       "0    (orbit, 0.8593956232070923)       (pick, 0.9560073614120483)   \n",
       "1   (starts, 0.8581326007843018)     (refute, 0.9553372859954834)   \n",
       "2  (heavens, 0.8402745127677917)    (escapes, 0.9544968605041504)   \n",
       "3    (night, 0.7679082751274109)  (literally, 0.9532182216644287)   \n",
       "4     (luke, 0.7593796253204346)       (trap, 0.9523584842681885)   \n",
       "\n",
       "                         physics                        computer  \\\n",
       "0  (pihatie, 0.7993094325065613)  (carnegie, 0.9562980532646179)   \n",
       "1     (rule, 0.7873783707618713)      (dept, 0.9539111256599426)   \n",
       "2   (boston, 0.7608155608177185)      (tech, 0.9470431208610535)   \n",
       "3     (game, 0.7527147531509399)    (mellon, 0.9383187890052795)   \n",
       "4    (ditto, 0.7299305200576782)   (virginia, 0.932227611541748)   \n",
       "\n",
       "                               study                              myth  \n",
       "0  (differences, 0.9717433452606201)      (illiad, 0.9435649514198303)  \n",
       "1         (draw, 0.9691357016563416)   (relevance, 0.9393407106399536)  \n",
       "2     (changing, 0.9681685566902161)       (jerry, 0.9353609085083008)  \n",
       "3    (happiness, 0.9679674506187439)  (undisputed, 0.9350335001945496)  \n",
       "4      (reflect, 0.9678720235824585)       (glory, 0.9347180128097534)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(get_similar_most_words(skip_win5_emb100, target_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the required embeddings papers (word2vec, Bert, optionally Elmo) and compare the approaches in less than one page, preferably in a table format. You are free to decide what aspects to compare.  For example, you can include information such as: learning model details, summary of word context approaches, corpus size requirements, computational requirements, ease of installation/use of source code, date of publication and number of google scholar citations :), … etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
