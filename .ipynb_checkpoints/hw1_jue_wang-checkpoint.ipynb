{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import glob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'alt.atheism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "d = []\n",
    "for file in glob.glob(\"*\"):\n",
    "    with open(file, 'r', encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "        data = f.read()\n",
    "    d.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = ''.join(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK without Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(contents, level = 'word'):\n",
    "    if level == 'word':\n",
    "        return word_tokenize(contents)\n",
    "    else:\n",
    "        return sent_tokenize(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization (both word and sentence level) in nltk for this corpus is : 4.29943323135376\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "words = nltk_tokenize(contents, level = 'word')\n",
    "sents = nltk_tokenize(contents, level = 'sent')\n",
    "print('Tokenization (both word and sentence level) in nltk for this corpus is :', time.time() - start_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(contents):\n",
    "    ps = PorterStemmer() \n",
    "    words = word_tokenize(contents)\n",
    "    stem = {}\n",
    "    for word in words:\n",
    "        stem[word] = ps.stem(word)\n",
    "#         stem.append(word+':'+ps.stem(word))\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming in nltk took: 8.434539794921875\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "stems = stemming(contents)\n",
    "print('Stemming in nltk took:', time.time() - start_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(contents):\n",
    "    words =  word_tokenize(contents)\n",
    "    nltk_pos = nltk.pos_tag(words)\n",
    "    return nltk_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging takes 18.19748091697693\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "nltk_pos = pos_tagging(contents)\n",
    "print (\"POS tagging takes %s\"%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Tokenization with Parallelization in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization (both word and sentence level) in nltk with tokenization for this corpus is 2.3760998249053955\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "words = Parallel(n_jobs=3)(delayed(nltk_tokenize)(i) for i in d)\n",
    "sents = Parallel(n_jobs=3)(delayed(nltk_tokenize)(i, 'sent') for i in d)\n",
    "print('Tokenization (both word and sentence level) in nltk with tokenization for this corpus is %s'%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(contents):\n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    nlp.max_length = 5000000\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    doc = nlp(contents)\n",
    "    sents = [sent.string.strip() for sent in doc.sents]\n",
    "    words = [token.text for token in doc]\n",
    "    return sents, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokneization takes 83.17936277389526\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sents, words = tokenization(contents)\n",
    "print (\"Tokneization takes %s\"%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy doesn't currently support stemming. We will use lemmatization instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(contents):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 5000000\n",
    "    doc = nlp(contents)\n",
    "    stem = {}\n",
    "    for token in doc:\n",
    "        stem[token] = token.lemma_\n",
    "        \n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization takes 88.59071898460388\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "stem = lemmatization(contents)\n",
    "print (\"Lemmatization takes %s\"%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(contents):\n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    nlp.max_length = 5000000\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    doc = nlp(contents)\n",
    "    words = [token.text for token in doc]\n",
    "    pos = {word:word.pos_ for word in doc}\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging takes 89.37744522094727\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pos = pos_tagging(contents)\n",
    "print (\"POS tagging takes %s\"%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Tokenization with Parallelization in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization (both word and sentence level) in Spacy with tokenization for this corpus is : 199.79079794883728\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "words = Parallel(n_jobs=3)(delayed(tokenization)(i) for i in d)\n",
    "print('Tokenization (both word and sentence level) in Spacy with tokenization for this corpus is :', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGEX Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p00261@psilink.com', '930416.141520.7h1.rusnews.w165w@mantis.co.uk', '2944079995.1.p00261@psilink.com', 'usenet@worldlink.com', 'mathew@mantis.co.uk', 'decay@cbnewsj.cb.att.com', 'C63AEC.FB3@cbnewsj.cb.att.com', 'bissda.4.734849678@saturn.wwc.edu', 'madhausC5yD87.KIp@netcom.com', 'madhausC5yD87.KIp@netcom.com', 'madhaus@netcom.com', 'healta@saturn.wwc.edu', 'jimh@carson.u.washington.edu', '1r0rmtINNk5n@shelley.u.washington.edu', 'timmbake@mcl.ucsb.edu', 'a137490@lehtori.cc.tut.fi', 'hdq@cc.tut.fi', '1993Apr10.191100.16094@ultb.isc.rit.edu', '1993Apr10.191100.16094@ultb.isc.rit.edu', 'snm6394@ultb.isc.rit.edu', 'a137490@cc.tut.fi', 'aaron@minster.york.ac.uk', '735563729.1016@minster.york.ac.uk', '1993Apr21.171937.2489@daffy.cs.wisc.edu', 'mccullou@snake10.cs.wisc.edu', 'west@next02cville.wam.umd.edu', '1993Apr6.021635.20958@wam.umd.edu', 'usenet@wam.umd.edu', 'west@next02.wam.umd.edu', 'kmr4.1433.734039535@po.CWRU.edu', 'kmr4.1433.734039535@po.CWRU.edu', 'kmr4@po.CWRU.edu', '1993Apr5.163050.13308@wam.umd.edu', 'west@next02cville.wam.umd.edu', 'kmr4.1422.733983061@po.CWRU.edu', 'kmr4@po.CWRU.edu', '1993Apr5.025924.11361@wam.umd.edu', 'west@next02cville.wam.umd.edu', 'west@wam.umd.edu', 'keith@cco.caltech.edu', '1qnpe2INN8b0@gap.caltech.edu', '1q55ssINN84p@gap.caltech.edu', 's0g@fido.asd.sgi.com', '1q5klpINNee4@gap.caltech.edu', '27k@fido.asd.sgi.com', '1q8npiINNjlf@gap.caltech.edu', 'r05@fido.asd.sgi.com', '1ql8mdINN674@gap.caltech.edu', 'a91@fido.asd.sgi.com', 'livesey@solntze.wpd.sgi.com', 'mikec@sail.LABS.TEK.COM', '13674@sail.LABS.TEK.COM', '1r15rvINNh8p@ctron-news.ctron.com', 'king@ctron.com', 'kv07@IASTATE.EDU', 'sail.LABS.TEK.COM@RELAY.CS.NET', 'mikec@sail.LABS.TEK.COM', 'I3150101@dbstu1.rz.tu-bs.de', '16BB7C080.I3150101@dbstu1.rz.tu-bs.de', 'postnntp@ibr.cs.tu-bs.de', '16BB4C9F3.I3150101@dbstu1.rz.tu-bs.de', 'C5rACM.41q@darkside.osrhe.uoknor.edu', 'C5rACM.41q@darkside.osrhe.uoknor.edu', 'bil@okcforum.osrhe.edu', 'decay@cbnewsj.cb.att.com', '1993Apr15.192942.7104@cbnewsj.cb.att.com', 'bissda.4.734849678@saturn.wwc.edu', 'EDM.93Apr15104322@gocart.twisto.compaq.com', 'EDM.93Apr15104322@gocart.twisto.compaq.com', 'edm@twisto.compaq.com', 'bissda@saturn.wwc.edu', 'frank@D012S658.uucp', 'nnv@horus.ap.mchp.sni.de', '1993Apr15.181924.21026@dcs.warwick.ac.uk', '8mb@horus.ap.mchp.sni.de', '1993Apr24.165301.8321@dcs.warwick.ac.uk', '1993Apr24.165301.8321@dcs.warwick.ac.uk', 'simon@dcs.warwick.ac.uk', '8mb@horus.ap.mchp.sni.de', 'frank@D012S658.uucp', '1993Apr15.181924.21026@dcs.warwick.ac.uk', 'simon@dcs.warwick.ac.uk', 'odwyer@sse.ie', 'keith@cco.caltech.edu', '1qnot5INN80o@gap.caltech.edu', '1p9bseINNi6o@gap.caltech.edu', 'b6j@fido.asd.sgi.com', '1pigidINNsot@gap.caltech.edu', '1993Apr12.205840.20141@levels.unisa.edu.au', '9051467f@levels.unisa.edu.au', 'aaron@minster.york.ac.uk', '735563474.911@minster.york.ac.uk', '1993Apr22.015922.7418@daffy.cs.wisc.edu', 'mccullou@snake2.cs.wisc.edu', 'kmr4@po.CWRU.edu', 'kmr4.1445.734063687@po.CWRU.edu', 'kmr4.1433.734039535@po.CWRU.edu', '1993Apr6.021635.20958@wam.umd.edu', '1993Apr6.021635.20958@wam.umd.edu', 'west@next02cville.wam.umd.edu']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "email = re.findall(r'[A-Za-z0-9_\\-\\.]+\\@[A-Za-z0-9_\\-\\.]+\\.[A-Za-z0-9_\\-\\.]+', contents)\n",
    "\n",
    "print(email[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_1 = re.findall(r'[0-9]+/[0-9]+/[0-9]+', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3/18/93', '3/31/93', '3/31/93', '4/3/93']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_2 = re.findall(r'[0-9]{4}\\-[0-9]{2}\\-[0-9]{2}', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_3 = re.findall(r'\\d+ [JFAMSOND]\\w+ \\d+', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16 Apr 1993',\n",
       " '16 Apr 1993',\n",
       " '26 Apr 1993',\n",
       " '20 Apr 1993',\n",
       " '15 Apr 1993',\n",
       " '23 Apr 1993',\n",
       " '6 Apr 1993',\n",
       " '17 Apr 1993',\n",
       " '21 Apr 93',\n",
       " '21 Apr 1993']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_4 = re.findall(r'[JFAMSOND]\\w+ \\d+[th|st]+ \\d+', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['February 17th 1992', 'February 21st 1989']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = dates_1 + dates_2 + dates_3 + dates_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
